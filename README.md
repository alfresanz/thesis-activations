## Self-learning activation functions

My second master's thesis, which studies the performance of several feed-forward models when replacing the standard ReLU activation function by a learnable activation function (modeled by a single-layer fully connected network of varying hidden size), as well as the shape of the learned activation functions.

**Disclamer**: Still a work in progress, the code, although functional, is quite messy, specially in the head of main.py where the experiments to launch are defined.